Bootstrapping and choosing candidate split variables from a subset of all vars,Question 1,"X1, X2 are highly correlated / codependent.",Question 2,drop-column requires retraining the model each drop.,Question 3,Selenium,Question 4
bootstrap samples/bagged trees and using max_features (subset of all features),5,"X1 and X2 are highly codependent; when dropping one of the two columns, the other column makes up for it entirely",5,You have to re-train the model every time you drop a column,5,selenium,2
bag and bootstrap,3,They are codependent,5,"Need to retrain the model for each drop, so it is slower",5,selenium,2
bagged trees and forgetting features,5,X1 and X2 are codependent,5,We have to retrain the model for every feature drop,5,selenium,2
1) Bootstrapping and 2) using random subset of features for tree node splits.,5,They are likely co-dependent/ collinear.,5,Drop column requires training the model ptimes.,5,selenium,2
bootstrapping and random subset of features for each tree,4,they are codependent,5,needs to retrain the model for each feature,5,selenium,2
"train each tree on a randomly selected subset of the train data (bagged trees) and amnesia (train data ""forgets"" features as decision nodes are created)",5,X1 and X2 are codependent features (correlated but may not be correlated strictly linearly).,5,"Because drop-column importance uses brute force, it is very expensive since it retrains on the whole model each time (n times for n features).",5,selenium,2
Boostrapping data; Select part of the columns to find best split,5,X1 and X2 are codependent features.,5,"For each column, drop-column method need to fit a new model. Permutation method only fit once at the beginning.",5,requests,0
1) train different trees on different subsets of data 2) each tree considers different subsets of columns when determining split,4,they are codependent features,5,"because the model must be retrained for every drop in variable, permuation importance only trains model once",5,selenium,2
limit feature set and bootstrapping,5,X1 and X2 are codependent.,5,Model has to be retrained after dropping each column.,5,requests,0
first is amnesia which means showing some subset of features not all. second is bootstrapping which means training with randomly selected 2/3 training set (oob),4,We can say X1 and X2 might be codependent variables.,5,Because drop column importance mechanism trains model each time after dropping features one by one. In other words we use brute force.,5,webdriver from Selenium,0
1. bagging 2. random subspace,3,X1 and X2 are codependent.,5,Because permutation importance doesn't need to retrain the model.,5,selenium,2
bootstrapping and max features,5,X1 and X2 are the same feature. (or almost the same).,0,For drop column importance you have to fit a new model every time you drop a column. In permutation importance you don't have to.,5,selenium,2
Bagging (bootstrapping and aggregating) and amnesia (deliberating making training forget soe features while creating decision nodes),5,X2 is duplicate of X1,3,"Because drop-column requires retraining the model p time for p features, whereas permutation importance does not.",5,selenium,2
bagging and restricting available features at each decision node,5,The features have a codependent relationship.,5,"Drop-column requires you to retrain the model for every feature, permutation does not.",5,selenium,2
Training trees on bootstrap samples and choosing max number of features per split,5,They are highly correlated.,5,It retrains a model in every iteration.,5,selenium,2
"bootstapped verison of the original training set, forget some available features",5,X1 and X2 are codependent,5,permutation importance don't need to retrain the model,5,selenium,2
"bootstrapping decision trees and inducing ""amnesia"" via random selection for a subset of features during decision splits",5,The two features are codependent!,5,You have to retrain the model for each dropped feature in drop-column importance (as opposed to permutation).,5,selenium,2
Amnesia and bootstrapping,4,X1 and X2 are highly correlated.,5,For drop column importance you have to refit the model for each dropped column. Refitting is not necessary for permutation importance.,5,webdriver,0
randomly choose a portion of features and bootstrap,5,X1 and X2 are highly correlated,5,"drop-column approach retrains the model p times for p features, permutation approach no need to retrain the model",5,selenium,2
randomness in features and randomness in samples used to train pre tree,5,X1 and X2 are highly correlative to each other.,5,Drop columns retrain model p time to test p features important but permutation not doing retrain only shuffle values within the column.,5,selenium,2
Use bagged trees and each bagged tree only sees a subset of features when creating decision nodes,4,X1 and X2 are codependent features.,5,Need to retrain the model every time after dropping each feature,5,selenium,2
bootstrap training records and randomly select max number of features,5,They are codependent features.,5,Because drop-column importance function refits the model every time after dropping one feature.,5,selenium,2
Bootsrapped rows; mode/mean voting for all trees,3,X1 and X2 are multicollinear,4,Drop-column retrains the model for every feature; Permutation importance doesn't have to,5,selenium,2
"bagging features to make trees train on diff features, bootstrapping the training data so that trees are not trained on same data",3,X1 and X2 are highly correlated or codependent and hence dropping one column did not affect the model.,5,"In drop column importance, we refit the model in every iteration which makes it slow.",5,selenium,2
1.subset features for each tree and randomly choose split value 2. bootstrapping,4,they are highly correlated,5,drop-column importance needs to retrain the model everytime you drop a column,5,selenium,2
Using Bagged Trees and restrict number of features when splitting the decision node,5,X1 and X2 are codependent features.,5,Drop column has to retrain the model p times for p features and Permutation importance there is no need to retrain the model,5,selenium,2
Bootstrapping data for each tree and restricting a number of random features for each decision node split in each tree.,5,X1 and X2 are codependent features,5,The implementation of drop-column importances requires you to retrain the model for each column you drop while the implementation of permutation importance only needs to be trained once.,5,selenium,2
Further weakening the tree and restrict the available features,3,X1 and X2 are codependent.,5,because it needs to train the whole model again.,5,selenium,2
Limit the amount of data seen by tree and number of features.,5,Codependent features,5,drop-column importance re-fit the model but permutation importance don‚Äôt need to re-fit the model again.,5,selenium,2
Bootstrapping & Restrict the available features to each tree,4,X1 and X2 are codependent features.,5,Because we need to re-train the model everytime after dropping a column,5,selenium,2
bootstrapping and randomly selecting a subset of candidate features for each split,5,X1 and X2 are codependent,5,Must retrain the model for each feature dropped,5,selenium,2
"Restrict the available features when searching for split (amnesia), and bagging",5,X1 and X2 are codependent features.,5,Drop-column importance is expensive as you need to retrain the model p times for p features.,5,selenium,2
Bootstrapping and only using a random subset of features to train each tree,4,They are likely codependent features,5,Have to retrain model for each feature in drop-column importance. Permutation importance only has to train one model.,5,selenium,2
1) randomly generating new training data (with replacement) for each tree 2) Limiting the number of features that each tree can consider at each split,5,They are related and X2's relation to X1 could be masking X1's importance in the model.,5,Drop column importance has to retrain the model multiple times (number of features in model).,5,selenium,2
restrict available features and bootstrap dataset.,5,they are a codependent relationship.,0,Drop-column importance has to retrain the model every time.,5,selenium,2
train with bagged trees and subset of features,5,X1 and X2 could be codependent features,5,We do not need to retrain the model by using permutation.,5,selenium,2
"Bootstrapping, limiting max candidate features tried per split",5,The two variables are likely codependent.,5,Model has to be retrained for when each of the columns is dropped. Permutations don't require retraining.,5,flask,0
"Using either bootstrapping or subsampling, and also using a subset of features",5,X1 and X2 are codependent,5,Drop-column importance requires retraining for each column while permutation importance does not,5,requests.get,0
"rf uses bootstrapped subset of data row and select subset of features, so tree see somewhat different data to help them be independant thinker.",5,they might be high co-linearly related.,5,since drop-column importance needs to fit data p times for p features. While permutation importance does not.,5,selenium,2
bootstrapping and max features,5,They are likely codependent,5,Drop-column requires training the model twice every iteration. Permutation importance only trains once,3,selenium,2
1. bootstraping; 2. ramdomly select limited number of features for individual tree,4,They are equivalent (duplicated features) or extremely highly correlated.,5,"In the drop-column importance algorithm, the model needs to be retrained.",5,selenium,2
bootstrapping and feature amnesia,5,they are codependent,5,drop column requires retraining of the model while permutation importance does not,5,selenium,2
Bagging (individual trees see less data) + Feature Amnesia (Individual trees get m random features at each split) m < p features,5,X1 and X2 are probably codependent. Codependent features often result in 0 for drop-column importances.,5,It requires that we retrain model p times (where p = # of features).,5,selenium,2
bootstrap aggregation; randomly select a part of features when splitting decision nodes,5,They are codependent.,5,It retrains the model every time when testing a single feature.,4,selenium,2
Bagging and taking a random sample of predictors,5,They are codependent.,5,Drop column importance requires repeatedly retraining the model whereas permutation importance does not. It only requires running the permuted sample through the same model.,5,selenium,2
1. Bagged trees 2. randomly training on subset of features (max features),5,X1 and X2 are codependent features.,5,"Drop-column importance requires retraining the model with each drop, while permutation importance simply shuffles the column and runs that back through the model.",5,selenium,2
Bootstrapping and selecting random m features,5,X1 and X2 are codependent,5,"because it retrains the model x times for x features, permutation - no need to retrain",5,selenium,2
Bootstrapping and Max Features (pick a random subset of columns to try for each split).,5,X1 and X2 are codependent.,5,In drop column we have to retrain the model for each column we drop in permutation we do not need to retrain each time.,5,selenium,2
Bootstrapping and restricting available features to look at each decision node,5,They are highly codependent,5,We have to refit the model every time we drop column,5,selenium,2
"1. Bootstrapping (train on random subset of data), 2. Restrict available features on split decision",5,X1 and X2 (almost) perfectly depend on each other,5,"Each column-drop step requires a retrain, permutation can use the same fitted model",5,selenium,2
bagging and randomly select part of features to split,5,they are codependent,5,"drop-column importance retrain the model p times for p features, but permutation don't need to retrain.",5,selenium,2
bagging (sampling data with replacement repeatedly) and having them forget some features,5,They are either highly correlated or uncorrelated,5,requires running the model num_col times wheras perm_imp runs once and recalculates metric num_col times,5,selenium,2
bootstrap aggregation and random subspace methodz`,3,X1 and X2 are dependent on each other.,5,Permutation importance does not require model re-training.,5,selenium,2
use a subset of columns and bootstrap data points,5,X1 and X2 are codependent.,5,We need to refit the model each time we drop a column.,5,selenium,2
Restrict max features at each decision node and bootstrapping.,5,We can say the X1 and X2 may be codependent features.,5,"Drop-column importance must retrain model x times if there are x number of features, while permutation does not need to retrain the model.",5,selenium,2
bootstrap aggregation and randomly selection of features,5,They could be codependent features.,5,Because drop-column re-fit the model per column feature.,5,selenium,2
bagging and restrict the available features,5,X1 and X2 are codependent features.,5,Because it has to retrain the model each time we drop a column.,5,selenium,2
1) bootstrapping/bagging: random sampling from training dataset 2) randomly forget features when creating decision tree nodes,5,"X1, X2 they are codependent or duplicated.",5,you have to retrain the model every time you drop a column,5,selenium,2
"bootstrapping, limiting available features when searching for a decision node split",5,X1 and X2 are correlated,5,we retrain the model everytime we drop a column,5,webdriver,0
bootstrap samples for each tree and bag/ensemble trees only with max_features not all features,5,X1 and X2 are codependent,5,"Drop-column has to retrain the model with new columns of each drop, but permutation don't",5,selenium,2
bootstrap aggregating/ bagging and random subspace method,3,X2 introduces high levels of noise that de correlates X1,0,drop-column requires retraining the entire data and is expensive and slow but permutation importance does not need retraining and hence relatively faster.,5,selenium,2
It randomly selects a fixed number of features for each tree and train the tree using random sample of data.,4,They are closely related (codependent) to each other.,5,"Every time we drop a column, we need to refit the model again and then estimate. But permutation importance is estimated without refitting the model.",5,selenium,2
Bagging (bootstrap aggregating) and selecting from random subset of columns at each decisionNode,5,X1 and X2 are codependent,5,Requires retraining the model for each dropped column.,5,selenium,2
bagging trees and have max features cap on available features,5,X1 and X2 are codependent.,5,We retrain the model for each column dropped.,5,selenium,2
"bootstrapping and forgetting features - ""amnesia""",5,Could be co-dependent (longitude and latitude)- drop columns often results in very low importance for co-dependent features.,5,"""Drop-column"" is very expensive- must retrain the model p times for p features,
which is not necessary for permutation importance.",5,selenium,2
Random forests use bootstrapping and amnesia in feature selection strategy to de-correlate trees.,5,X2 is codependent with X1.,5,"Drop-column retrains the model, premutation does not",5,selenium,2
Bootstrapping and limiting the number of candidate features for splitting (max_features hyperparameter),5,They are codependent and share the importance,5,Drop column requires us to do re-fitting which is expensive and slower,5,selenium,2
bootstrap and randomly choose m features out of p,5,X_1 and X_2 are severely codependent,5,we need to refit the model every time we drop a column,5,selenium,2
"bootstrapping/bagging, restrict features per split",5,codependent features,5,"It's retraining the model for the number of features times, while permutation has no need to retrain.",5,selenium,2
Restrict available features to each tree and Bootstrapping,4,They are very highly correlated.,5,"After dropping any columns, The model has to be re-trained.",5,selenium,2
"bootstrap, bagging",3,one of them is duplicated. X1 is duplicate of X2 or X2 is duplicate of X1,3,Because it has to retrain the model n times for n features,5,selenium,2
Bagged trees created by Bootstrapping and Random forests - forgetting some features,5,X1 and X2 are highly codependent features.,5,Drop-column importance fits the model again on the training data every time a column is dropped. Permutation importance doesn't fit the data again.,5,selenium,2
generating many trees and and aggregate the prediction of each tree to a single model prediction,0,X1 and X2 have correlation with each other,5,Since drop-column importance need to drop each column and retrain model to see the result. While permutation importance doesnt require to retrain model,5,rerquests,0
bootstrapping and restricting features (amnesia),5,X1 and X2 are highly correlated.,5,We have to retrain the model p (number of features) times in order to get the importances while permutation importance is trained once.,5,selenium,2
Bagging,3,X2 creates noise for X1 creating decorrelation,0,Drop-column importance requires retraining on the model.,5,selenium,2
use bootstrap and select subset of features (forgetful),5,X1 and X2 are codependent,5,it retrains model p (number of variables) times,5,selenium,2
Bootstrapping and bagged trees,3,X1 and X2 are codependent features,5,"drop-column importance needs to retrain the model, but permutation importance doesn't.",5,selenium,2
set max features and use out-of-bag technique,3,They are codependent features,5,drop-column needs to retrain the model but permutation doesn't need,5,selenium,2
Train each tree on a bootstrapped sample and at each node pick a split feature from a random subset of all features,5,They are codependent features,5,"Each time we drop a column, we have to re-train the entire model before calculating validation metric. With permutation importance, we can just permutate the column and calculate the validation metric.",5,selenium,2
"Bootstrapping/bagging (reduce available dataset rows), and setting maximum features (selecting k << p columns)",5,X1 and X2 are probably codependent,5,You have to retrain the model p times (once for every feature). Permutation importance does not involve retraining.,5,Using the python library Selenium,2
